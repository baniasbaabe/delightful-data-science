{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest on steroids: Parallelize your test with pytest-xdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your test suite takes a very long time to run in Python?\n",
    "\n",
    "With `pytest-xdist` installed, you can run your tests in parallel.\n",
    "\n",
    "Specify the number of CPUs you want to use with `--numprocesses`.\n",
    "\n",
    "This allows to speed up your test executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-xdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --numprocesses 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the order of your tests with pytest-randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is interesting to run your test cases in a random order to check if there is any test case order dependency.\n",
    "\n",
    "By running test cases in a random order, you can check if any test cases are dependent on the order of execution.\n",
    "\n",
    "To do that in Python, try `pytest-randomly`.\n",
    "\n",
    "`pytest-randomly` is a pytest plugin to randomly shuffle the order of your tests.\n",
    "\n",
    "It can be helpful in addition to other test strategies.\n",
    "\n",
    "- The output tells you which random seed was used.\n",
    "- If you want to use that seed again, use the flag `--randomly-seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test coverage with pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to measure the test coverage of your code in Python?\n",
    "\n",
    "Try `pytest-cov`.\n",
    "\n",
    "`pytest-cov` is a pytest plugin producing test coverage reports for you.\n",
    "\n",
    "You can see how many statements in your code are covered in a nicely generated report.\n",
    "\n",
    "Below you can see an example of how to use `pytest-cov`.\n",
    "\n",
    "- With the --ùêúùê®ùêØ flag, you set the path to the module or package you want to measure coverage for.\n",
    "\n",
    "-You can also specify the minimum required test coverage percentage using the\n",
    "`--cov-fail-under` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --cov=src --cov-fail-under=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-------------------- coverage: ... ---------------------\n",
    "Name                 Stmts   Miss  Cover\n",
    "----------------------------------------\n",
    "src/__init__             2      0   100%\n",
    "src/module1.py         257     13    94%\n",
    "src/module2.py         100      0   100%\n",
    "----------------------------------------\n",
    "TOTAL                  359     13    97%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Test your plots with pytest-mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to test your plots in Python?\n",
    "\n",
    "Nowadays, you can test everything.\n",
    "\n",
    "Functions, classes, websites, ‚Ä¶\n",
    "\n",
    "But how to make sure to check if your plots are correctly generated in Python?\n",
    "\n",
    "Try `pytest-mpl`!\n",
    "\n",
    "`pytest-mpl` is a Pytest plugin for testing plots created using Matplotlib.\n",
    "\n",
    "It allows you to compare the output of your Matplotlib plots with expected results by automatically saving them as images and comparing them with pre-saved \"baseline\" images using image diffing techniques.\n",
    "\n",
    "Below, you can see an example of how to use `pytest-mpl`.\n",
    "\n",
    "- You have to mark the function where you want to compare images with `@pytest.mark.mpl_image_compare`.\n",
    "- Provide the `--mpl-generate-path` option with the name of the directory where the baseline images should be saved.\n",
    "- To test if the images are the same, provide the `--mpl` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile.py\n",
    "import pytest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@pytest.mark.mpl_image_compare()\n",
    "def test_plotting_line():\n",
    "    fig = plt.figure()\n",
    "    plt.plot([1,2,3,4,5,6,7,8])\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -k test_plotting_line --mpl-generate-path=baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantly show errors in your Test Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your tests with `pytest`, it will run all test cases and show you the results at the end.\n",
    "\n",
    "But you don't want to wait until the end to see if some tests failed. You want to see the failed tests instantly.\n",
    "\n",
    "`pytest-instafail` is a plugin which shows failures immediately instead of waiting until the end.\n",
    "\n",
    "See below for an example of how to install and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-instafail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --instafail"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}