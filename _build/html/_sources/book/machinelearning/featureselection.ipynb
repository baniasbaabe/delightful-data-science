{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Variance Inflation Factor (VIF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to detect Multicollinearity?\n",
    "\n",
    "Multicollinearity is a statistical phenomenon that occurs when two or more predictor variables in a multiple regression model are highly correlated.\n",
    "This can lead to unstable and inconsistent coefficients, making it difficult to interpret the model's results.\n",
    "\n",
    "To measure multicollinearity, you can use the ùêïùêöùê´ùê¢ùêöùêßùêúùêû ùêàùêßùêüùê•ùêöùê≠ùê¢ùê®ùêß ùêÖùêöùêúùê≠ùê®ùê´ (VIF)\n",
    "\n",
    "VIF is defined as the ratio of the variance of an estimated regression coefficient to the variance of the coefficient when the predictor variables are not correlated.\n",
    "\n",
    "A high VIF value (VIF > 5 or > 10) indicates that multicollinearity is present and may be a problem.\n",
    "\n",
    "To calculate the VIF for a predictor variable, you can fit a multiple regression model with all of the predictor variables except for that variable, and then calculate the VIF using the following formula:\n",
    "\n",
    "VIF = 1 / (1 - R^2)\n",
    "\n",
    "where R^2 is the coefficient of determination from the regression model.\n",
    "\n",
    "You can repeat this process for each predictor variable and compare the VIF values to determine which predictor variables contribute to multicollinearity.\n",
    "\n",
    "Now you could drop the predictor variables with high VIF and calculate the VIF for the remaining again to see, how their VIF has changed.\n",
    "\n",
    "Below you can see how to calculate VIF with ùê¨ùê≠ùêöùê≠ùê¨ùê¶ùê®ùêùùêûùê•ùê¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Predictor\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Predictor        VIF\n",
    "0       CRIM   2.100373\n",
    "1         ZN   2.844013\n",
    "2      INDUS  14.485758\n",
    "3       CHAS   1.152952\n",
    "4        NOX  73.894947\n",
    "5         RM  77.948283\n",
    "6        AGE  21.386850\n",
    "7        DIS  14.699652\n",
    "8        RAD  15.167725\n",
    "9        TAX  61.227274\n",
    "10   PTRATIO  85.029547\n",
    "11         B  20.104943\n",
    "12     LSTAT  11.102025\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for new categories in test set with `Deepchecks`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check if your test set has new categories when training a Machine Learning Model.\n",
    "\n",
    "Some algorithms like CatBoost can handle unknown categories.\n",
    "\n",
    "But when you have more and more unknown categories, it will harm your model.\n",
    "\n",
    "Instead, check the mismatch beforehand with `Deepchecks‚Äô` `CategoryMismatchTrainTest`.\n",
    "\n",
    "It will show you if there are new categories so you can handle them appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks.train_test_validation import CategoryMismatchTrainTest\n",
    "checker = CategoryMismatchTrainTest()\n",
    "\n",
    "X_train = pd.DataFrame([[\"A\", \"B\", \"C\"], [\"B\", \"B\", \"A\"]], columns=[\"Col1\", \"Col2\", \"Col3\"])\n",
    "X_test = pd.DataFrame([[\"B\", \"C\", \"D\"], [\"D\", \"A\", \"B\", ]], columns=[\"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "checker.run(X_train, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Permutation Importance with `eli5`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Permutation Importance method to obtain feature importances.\n",
    "\n",
    "Permutation Importance calculates feature importance by randomly shuffling the values of a feature and observing how the model's performance changes.\n",
    "\n",
    "In comparison to Feature Importance, Permutation Importance works for every model (and not only for tree-based models).\n",
    "\n",
    "With `eli5`, you can calculate Permutation Importance with ease.\n",
    "\n",
    "`show_weights()` will show you the features which hurts the performance the most, so they are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris() \n",
    "X = iris.data \n",
    "target = iris.target \n",
    "names = iris.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target)\n",
    "\n",
    "svc = SVC().fit(X_train, y_train)\n",
    "perm = PermutationImportance(svc).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names= [\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Most Predictive Variables for Your Target Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know about Correlation. But do you know the Predictive Power Score?\n",
    "\n",
    "Predictive Power Score (PPS) is a data-type-agnostic score that can detect linear and non-linear relationships between two columns, with an output ranging from 0 to 1.\n",
    "\n",
    "So, a PPS of 1 means Column A is very likely to predict the values of Column B.\n",
    "\n",
    "You can use it to identify which variables are most useful to predict the target variable.\n",
    "\n",
    "In Python, you can use the `ppscore` library.\n",
    "\n",
    "It can calculate the PPS of all the features in a dataframe against a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ppscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppscore.predictors(df, \"target\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection at Scale with `mrmr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to do Feature Selection automatically?\n",
    "\n",
    "Try `mrmr`.\n",
    "\n",
    "`mrmr` (minimum-Redundancy-Maximum-Relevance) is a minimal-optimal feature selection algorithm at scale.\n",
    "\n",
    "It means `mrmr` will find the smallest relevant subset of features your ML Model needs.\n",
    "\n",
    "`mrmr` supports common tools like Pandas, Polars and Spark.\n",
    "\n",
    "See below how we want to select the best K features.\n",
    "\n",
    "The output is a ranked list of the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mrmr_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "X, y = make_classification(n_samples = 1000, n_features = 50, n_informative = 10, n_redundant = 40)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "\n",
    "selected_features = mrmr_classif(X=X, y=y, K=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bf0f8d5625db32e314b5bdaf50a44046044c99ae376da8e1ac5bc25f06b01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
