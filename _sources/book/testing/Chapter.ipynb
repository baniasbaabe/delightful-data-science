{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest on steroids: Parallelize your test with `pytest-xdist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your test suite takes a very long time to run in Python?\n",
    "\n",
    "With `pytest-xdist` installed, you can run your tests in parallel.\n",
    "\n",
    "Specify the number of CPUs you want to use with `--numprocesses`.\n",
    "\n",
    "This allows you to speed up your test executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-xdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --numprocesses 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the order of your tests with `pytest-randomly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is interesting to run your test cases in a random order to check if there is any test case order dependency.\n",
    "\n",
    "By running test cases in a random order, you can check if any test cases are dependent on the order of execution.\n",
    "\n",
    "To do that in Python, try `pytest-randomly`.\n",
    "\n",
    "`pytest-randomly` is a pytest plugin to randomly shuffle the order of your tests.\n",
    "\n",
    "It can be helpful in addition to other test strategies.\n",
    "\n",
    "- The output tells you which random seed was used.\n",
    "- If you want to use that seed again, use the flag `--randomly-seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test coverage with `pytest-cov`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to measure the test coverage of your code in Python?\n",
    "\n",
    "Try `pytest-cov`.\n",
    "\n",
    "`pytest-cov` is a pytest plugin producing test coverage reports for you.\n",
    "\n",
    "You can see how many statements in your code are covered in a nicely generated report.\n",
    "\n",
    "Below you can see an example of how to use `pytest-cov`.\n",
    "\n",
    "- With the --ùêúùê®ùêØ flag, you set the path to the module or package you want to measure coverage for.\n",
    "\n",
    "-You can also specify the minimum required test coverage percentage using the\n",
    "`--cov-fail-under` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --cov=src --cov-fail-under=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-------------------- coverage: ... ---------------------\n",
    "Name                 Stmts   Miss  Cover\n",
    "----------------------------------------\n",
    "src/__init__             2      0   100%\n",
    "src/module1.py         257     13    94%\n",
    "src/module2.py         100      0   100%\n",
    "----------------------------------------\n",
    "TOTAL                  359     13    97%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Test your plots with `pytest-mpl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to test your plots in Python?\n",
    "\n",
    "Nowadays, you can test everything.\n",
    "\n",
    "Functions, classes, websites, ‚Ä¶\n",
    "\n",
    "But how to make sure to check if your plots are correctly generated in Python?\n",
    "\n",
    "Try `pytest-mpl`!\n",
    "\n",
    "`pytest-mpl` is a Pytest plugin for testing plots created using Matplotlib.\n",
    "\n",
    "It allows you to compare the output of your Matplotlib plots with expected results by automatically saving them as images and comparing them with pre-saved \"baseline\" images using image diffing techniques.\n",
    "\n",
    "Below, you can see an example of how to use `pytest-mpl`.\n",
    "\n",
    "- You have to mark the function where you want to compare images with `@pytest.mark.mpl_image_compare`.\n",
    "- Provide the `--mpl-generate-path` option with the name of the directory where the baseline images should be saved.\n",
    "- To test if the images are the same, provide the `--mpl` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile.py\n",
    "import pytest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@pytest.mark.mpl_image_compare()\n",
    "def test_plotting_line():\n",
    "    fig = plt.figure()\n",
    "    plt.plot([1,2,3,4,5,6,7,8])\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -k test_plotting_line --mpl-generate-path=baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantly show errors in your Test Suite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your tests with `pytest`, it will run all test cases and show you the results at the end.\n",
    "\n",
    "But you don't want to wait until the end to see if some tests failed. You want to see the failed tests instantly.\n",
    "\n",
    "`pytest-instafail` is a plugin which shows failures immediately instead of waiting until the end.\n",
    "\n",
    "See below for an example of how to install and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-instafail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --instafail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit pytest's output to a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to reduce pytest‚Äôs chatty output?\n",
    "\n",
    "Try pytest-tldr.\n",
    "\n",
    "pytest-tldr is a pytest plugin to limit the output to the most important things.\n",
    "\n",
    "A nice plugin if you don‚Äôt want to be annoyed by pytest‚Äôs default output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest-tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -v # -v for detailed but clean output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property-based Testing with `hypothesis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for a smarter way to test your Python code?\n",
    "\n",
    "Use `hypothesis`.\n",
    "\n",
    "With `hypothesis`, you define properties your code should uphold, and it generates diverse test cases, uncovering edge cases and unexpected bugs.\n",
    "\n",
    "I encourage you to look into their documentation since it‚Äôs really upgrading your testing game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import given, strategies as st\n",
    "\n",
    "@given(st.integers(), st.integers())\n",
    "def test_addition_commutative(a, b):\n",
    "    assert a + b == b + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking Dependencies with `pytest-mock`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing is an essential part of Software projects.\n",
    "\n",
    "\n",
    "\n",
    "Especially unit testing, where you test the smallest piece of code that can be isolated.\n",
    "\n",
    "\n",
    "\n",
    "They should be independent, and fast & cheap to execute.\n",
    "\n",
    "\n",
    "\n",
    "But, what if you have some dependencies like API calls or interactions with databases and systems?\n",
    "\n",
    "\n",
    "\n",
    "Here's where mocking comes into play.\n",
    "\n",
    "\n",
    "\n",
    "Mocking allows you to replace dependencies and real objects with fake ones which mimic the real behavior.\n",
    "\n",
    "\n",
    "\n",
    "So, you don't have to rely on the availability of your API, or ask for permission to interact with a database, but you can test your functions isolated and independently.\n",
    "\n",
    "\n",
    "\n",
    "In Python, you can perform mocking with `pytest-mock`, a wrapper around the built-in mock functionality of Python.\n",
    "\n",
    "\n",
    "\n",
    "See the example below, where we mock the file removal functionality. We can test it without deleting a file from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class UnixFS:\n",
    "    @staticmethod\n",
    "    def rm(filename):\n",
    "        os.remove(filename)\n",
    "def test_unix_fs(mocker):\n",
    "    mocker.patch('os.remove')\n",
    "    UnixFS.rm('file')\n",
    "    os.remove.assert_called_once_with('file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Datetime Module For Testing with `freezegun`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to test functions with datetime,\n",
    "\n",
    "Consider using `freezegun` for Python.\n",
    "\n",
    "`freezegun` mocks the datetime module which makes testing deterministic.\n",
    "\n",
    "See below how we can specify a date and freeze the return value of datetime.datetime.now()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install freezegun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freezegun import freeze_time\n",
    "import datetime\n",
    "\n",
    "@freeze_time(\"2015-02-20\")\n",
    "def test():\n",
    "    assert datetime.datetime.now() == datetime.datetime(2015, 2, 20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
