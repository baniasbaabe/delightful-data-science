{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tips and Tricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage your Webdrivers with `webdriver-manager`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `Selenium` with Python, you probably did the following:\n",
    "\n",
    "- Download Chromedriver Binary\n",
    "- Unzip it\n",
    "- Set the path to the driver\n",
    "\n",
    "This is annoying.\n",
    "\n",
    "- The Path can be changed\n",
    "- You have to somehow manage those browser drivers for each OS\n",
    "- Check if new updates for drivers are released\n",
    "\n",
    "Instead of doing this manually, use `webdriver-manager`.\n",
    "\n",
    "It makes managing binaries for different browsers easy.\n",
    "\n",
    "`webdriver-manager` downloads binaries automatically for you.\n",
    "\n",
    "So you don't have to go through the pain of doing it manually.\n",
    "\n",
    "\n",
    "To use it in your project, see the example below. It's straightforward and saves you time and energy.\n",
    "\n",
    "Especially when you integrate `Selenium` in your CI/CD Pipeline.\n",
    "\n",
    "By default, `webdriver-manager` installs the latest version.\n",
    "\n",
    "But you can also define a specific version of the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome('path/to/driver.exe')\n",
    "\n",
    "# New way with webdriver-manager and Selenium 4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# New way with webdriver-manager and Selenium 3\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Use specific version\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager(\"<your_version>\").install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up your Scraping with disabling image loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to speed up your web scraper?\n",
    "\n",
    "Disable image loading!\n",
    "\n",
    "Disabling image loading while scraping is a great way to speed up your scraper.\n",
    "\n",
    "You are wasting a lot of connection bandwidth.\n",
    "\n",
    "To disable image loading in Selenium, you only have to set one option (like below).\n",
    "\n",
    "This will save you time and money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.get(\"https://www.instagram.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered Web Scraper with `scrapegraph-ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to let AI scrape your website?\n",
    "\n",
    "Use `scrapegraph-ai`.\n",
    "\n",
    "This library uses LLM and direct graph logic to scrape websites by only providing the information you need.\n",
    "\n",
    "See below where we give it a prompt and an URL.\n",
    "\n",
    "It also supports multi-page scraper that extracts information from the top n search results of a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapegraphai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": OPENAI_API_KEY,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\":0,\n",
    "    },\n",
    "    \"verbose\":True,\n",
    "}\n",
    "\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "    prompt=\"List me all the projects with their descriptions.\",\n",
    "    source=\"https://perinim.github.io/projects/\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "'''\n",
    "Output\n",
    "{\n",
    "  \"projects\": [\n",
    "    {\n",
    "      \"title\": \"Rotary Pendulum RL\",\n",
    "      \"description\": \"Open Source project aimed at controlling ...\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"DQN Implementation from scratch\",\n",
    "      \"description\": \"Developed a Deep Q-Network algorithm to train a ...\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Multi Agents HAED\",\n",
    "      \"description\": \"University project which focuses ....\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Wireless ESC for Modular Drones\",\n",
    "      \"description\": \"Modular drone architecture ...\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl and Scrape Any Website with LLMs and `crawl4ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping websites is hard.\n",
    "\n",
    "Fortunately, LLM-powered scraping & crawling is now possible.\n",
    "\n",
    "**Crawl4AI** is a Python tool to scrape and crawl data from any website with LLMs, allowing structured data extraction and markdown generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crawl4ai\n",
    "!pip install crawl4ai-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawl4ai.extraction_strategy import *\n",
    "from crawl4ai.crawler_strategy import *\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "url = r\"https://openai.com/api/pricing/\"\n",
    "\n",
    "class OpenAIModelFee(BaseModel):\n",
    "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
    "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
    "    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\"\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=url,\n",
    "            word_count_threshold=1,\n",
    "            extraction_strategy=LLMExtractionStrategy(\n",
    "                provider=\"groq/llama-3.1-70b-versatile\",\n",
    "                api_token=os.getenv(\"GROQ_API_KEY\"),\n",
    "                schema=OpenAIModelFee.model_json_schema(),\n",
    "                extraction_type=\"schema\",\n",
    "                instruction=\"From the crawled content, extract all mentioned model names along with their \"\n",
    "                \"fees for input and output tokens. Make sure not to miss anything in the entire content. \"\n",
    "                \"One extracted model JSON format should look like this: \"\n",
    "                '{\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        with open(\".data/data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result.extracted_content)\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
