{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Interactions Instead of Just Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Are you still using SHAP for explaining Machine Learning models?\n",
    "\n",
    "Then it's time to switch to 𝗦𝗛𝗔𝗣-𝗜𝗤.\n",
    "\n",
    "Shapley values became very popular due to distributing effects of features fairly.\n",
    "\n",
    "One problem with it: Standard SHAP compresses complex feature interactions into single values, obscuring how features truly influence predictions together.\n",
    "\n",
    "You're missing crucial insights about your model's behavior.\n",
    "\n",
    "Shapley Interactions, on the other side, enhance the traditional Shapley value approach by providing detailed measurements for every meaningful feature combination.\n",
    "\n",
    "Now, you will not get a single value per feature, but a number for each combination of features.\n",
    "\n",
    "𝗦𝗛𝗔𝗣-𝗜𝗤 (Shapley Interaction Quantification) is the Python package you need to explain your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shapiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapiq\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# set up an explainer with k-SII interaction values up to order 4\n",
    "explainer = shapiq.TabularExplainer(\n",
    "    model=model,\n",
    "    data=X,\n",
    "    index=\"k-SII\",\n",
    "    max_order=4\n",
    ")\n",
    "\n",
    "# explain the model's prediction for the first sample\n",
    "interaction_values = explainer.explain(X[0], budget=256)\n",
    "# analyse interaction values\n",
    "print(interaction_values)\n",
    "\n",
    ">> InteractionValues(\n",
    ">>     index=k-SII, max_order=4, min_order=0, estimated=False,\n",
    ">>     estimation_budget=256, n_players=8, baseline_value=2.07282292,\n",
    ">>     Top 10 interactions:\n",
    ">>         (0,): 1.696969079  # attribution of feature 0\n",
    ">>         (0, 5): 0.4847876\n",
    ">>         (0, 1): 0.4494288  # interaction between features 0 & 1\n",
    ">>         (0, 6): 0.4477677\n",
    ">>         (1, 5): 0.3750034\n",
    ">>         (4, 5): 0.3468325\n",
    ">>         (0, 3, 6): -0.320  # interaction between features 0 & 3 & 6\n",
    ">>         (2, 3, 6): -0.329\n",
    ">>         (0, 1, 5): -0.363\n",
    ">>         (6,): -0.56358890\n",
    ">> )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
