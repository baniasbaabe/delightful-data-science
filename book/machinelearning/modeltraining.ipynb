{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6603ef12",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e059c",
   "metadata": {},
   "source": [
    "## Compute Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a14ad",
   "metadata": {},
   "source": [
    "To handle class imbalance in Machine Learning, there are several methods.\n",
    "<br><br>\n",
    "One of them is adjusting the class weights. \n",
    "<br><br>\n",
    "By giving higher weights to the minority class and lower weights to the majority class, we can regularize the loss function.\n",
    "<br><br>\n",
    "Misclassifying the minority class will result in a higher loss due to the higher weight.\n",
    "<br><br>\n",
    "To incorporate class weights in Tensorflow, use `scikit-learn`'s `compute_class_weight` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfff72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "X, y = ...\n",
    "\n",
    "# will return an array with weights for each class, e.g. [0.6, 0.6, 1.]\n",
    "class_weights = compute_class_weight(\n",
    "  class_weight=\"balanced\",\n",
    "  classes=np.unique(y),\n",
    "  y=y\n",
    ")\n",
    "\n",
    "# to get a dictionary with {<class>:<weight>}\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = tf.keras.Sequential(...)\n",
    "model.compile(...)\n",
    "\n",
    "# using class_weights in the .fit() method\n",
    "model.fit(X, y, class_weight=class_weights, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1981d413",
   "metadata": {},
   "source": [
    "## Reset TensorFlow/Keras Global State"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8279453",
   "metadata": {},
   "source": [
    "In Tensorflow/Keras, when you create multiple models in a loop, you will need `tf.keras.backend.clear_session()`.\n",
    "\n",
    "Keras manages a global state, which includes configurations and the current values (weights and biases) of the models.\n",
    "\n",
    "So when you create a model in a loop, the global state gets bigger and bigger with every created model. To clear the state, ğğğ¥ ğ¦ğ¨ğğğ¥ will not work because it will only delete the Python variable.\n",
    "\n",
    "So `tf.keras.backend.clear_session()` is a better option. It will reset the state of a model and helps avoid clutter from old models.\n",
    "\n",
    "See the first example below. Each iteration of this loop will increase the size of the global state and of your memory.\n",
    "\n",
    "In the second example, the memory consumption stays constant by clearing the state with every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model():\n",
    "  model = tf.keras.Sequential(...)\n",
    "  return model\n",
    "\n",
    "# without clearing session\n",
    "for _ in range(20):\n",
    "  model = create_model()\n",
    "  \n",
    "# with clearing session\n",
    "for _ in range(20):\n",
    "  tf.keras.backend.clear_session()\n",
    "  model = create_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bd2797b",
   "metadata": {},
   "source": [
    "## Find dirty labels with cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc71d9b",
   "metadata": {},
   "source": [
    "Do you want to identify noisy labels in your dataset?\n",
    "\n",
    "Try `cleanlab` for Python.\n",
    "\n",
    "`cleanlab` is a data-centric AI package to automatically detect noisy labels and address dataset issues to fix them via confident learning algorithms.\n",
    "\n",
    "It works with nearly every model possible:\n",
    "\n",
    "- XGBoost\n",
    "- scikit-learn models\n",
    "- Tensorflow\n",
    "- PyTorch\n",
    "- HuggingFace\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cleanlab\n",
    "\n",
    "import cleanlab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "cl = cleanlab.classification.CleanLearning(clf)\n",
    "\n",
    "label_issues = cl.find_label_issues(X, y)\n",
    "\n",
    "print(label_issues.query('is_label_issue == True'))\n",
    "\n",
    "    is_label_issue  label_quality  given_label  predicted_label\n",
    "70            True           0.07            1                2\n",
    "77            True           0.01            1                2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec0393c",
   "metadata": {},
   "source": [
    "## Evaluate your Classifier with classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abaf8c",
   "metadata": {},
   "source": [
    "Would you like to evaluate your Machine Learning model quickly?\n",
    "\n",
    "Try `classification_report` from scikit-learn\n",
    "\n",
    "With `classification_report`, you can quickly assess the performance of your model.\n",
    "\n",
    "It summarizes Precision, Recall, F1-Score, and Support for each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c43a832",
   "metadata": {},
   "source": [
    "## Obtain Reproducible Optimizations Results in Optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206adcb1",
   "metadata": {},
   "source": [
    "Optuna is a powerful hyperparameter optimization framework that supports many machine learning frameworks, including TensorFlow, PyTorch, and XGBoost.\n",
    "\n",
    "But you need to be careful with reproducible results for hyperparameter tuning.tuple\n",
    "\n",
    "To achieve reproducible results, you need to set the seed for your Sampler.\n",
    "\n",
    "Below you can see how it is done for `TPESampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    ...\n",
    "    \n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ed7523",
   "metadata": {},
   "source": [
    "## Find bad labels with doubtlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1ddc1",
   "metadata": {},
   "source": [
    "Do you want to find bad labels in your data?\n",
    "\n",
    "Try `doubtlab` for Python.\n",
    "\n",
    "With `doubtlab`, you can define reasons to doubt your labels and take a closer look.\n",
    "\n",
    "Reasons to doubt your labels can be for example:\n",
    "\n",
    "- ğğ«ğ¨ğ›ğšğ‘ğğšğ¬ğ¨ğ§: When the confidence values are low for any label\n",
    "- ğ–ğ«ğ¨ğ§ğ ğğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§ğ‘ğğšğ¬ğ¨ğ§: When a model cannot predict the listed label\n",
    "- ğƒğ¢ğ¬ğšğ ğ«ğğğ‘ğğšğ¬ğ¨ğ§: When two models disagree on a prediction.\n",
    "- ğ‘ğğ¥ğšğ­ğ¢ğ¯ğğƒğ¢ğŸğŸğğ«ğğ§ğœğğ‘ğğšğ¬ğ¨ğ§: When the relative difference between label and prediction is too high\n",
    "\n",
    "So, identify your noisy labels and fix them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubtlab.ensemble import DoubtEnsemble\n",
    "from doubtlab.reason import ProbaReason, WrongPredictionReason\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Define reasons to check\n",
    "reasons = {\n",
    "    'proba': ProbaReason(model=model),\n",
    "    'wrong_pred': WrongPredictionReason(model=model),\n",
    "}\n",
    "\n",
    "# Pass reasons to DoubtLab instance\n",
    "doubt = DoubtEnsemble(**reasons)\n",
    "\n",
    "# Returns DataFrame with reasoning\n",
    "predicates = doubt.get_predicates(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44919184",
   "metadata": {},
   "source": [
    "## Get notified when your model is finished with training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998cfa5",
   "metadata": {},
   "source": [
    "Never stare at your screen, waiting for your model to finish training.\n",
    "\n",
    "Try `knockknock` for Python.\n",
    "\n",
    "`knockknock` is a library that notifies you when your training is finished.\n",
    "\n",
    "You only need to add a decorator.\n",
    "\n",
    "Currently, you can get a notification through 12 different channels\n",
    "like:\n",
    "\n",
    "- Email\n",
    "- Slack\n",
    "- Telegram\n",
    "- Discord\n",
    "- MS Teams\n",
    "\n",
    "\n",
    "Use it for your future model training and donâ€™t stick to your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install knockknock\n",
    "from knockknock import email_sender\n",
    "\n",
    "@email_sender(recipient_emails=[\"coolmail@python.com\", \"2coolmail@python.com\"], sender_email=\"anothercoolmail@python.com\")\n",
    "def train_model(model, X, y):\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c01896ba",
   "metadata": {},
   "source": [
    "## Get Model Summary in PyTorch with torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63bee6",
   "metadata": {},
   "source": [
    "Do you want a Model summary in PyTorch?\n",
    "\n",
    "Like in Keras with `model.summary()`?\n",
    "\n",
    "Use `torchinfo`.\n",
    "\n",
    "With `torchinfo`, you can get a model summary as you know it from\n",
    "Keras.\n",
    "\n",
    "Just add one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "class MyModel(torch.nn.Module)\n",
    "  ...\n",
    "  \n",
    "model = MyModel()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28))\n",
    "\n",
    "'''\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "Net                                      [16, 10]                  --\n",
    "â”œâ”€Sequential: 1-1                        [16, 4, 7, 7]             --\n",
    "â”‚    â””â”€Conv2d: 2-1                       [16, 4, 28, 28]           40\n",
    "â”‚    â””â”€BatchNorm2d: 2-2                  [16, 4, 28, 28]           8\n",
    "â”‚    â””â”€ReLU: 2-3                         [16, 4, 28, 28]           --\n",
    "â”‚    â””â”€MaxPool2d: 2-4                    [16, 4, 14, 14]           --\n",
    "â”‚    â””â”€Conv2d: 2-5                       [16, 4, 14, 14]           148\n",
    "â”‚    â””â”€BatchNorm2d: 2-6                  [16, 4, 14, 14]           8\n",
    "â”‚    â””â”€ReLU: 2-7                         [16, 4, 14, 14]           --\n",
    "â”‚    â””â”€MaxPool2d: 2-8                    [16, 4, 7, 7]             --\n",
    "â”œâ”€Sequential: 1-2                        [16, 10]                  --\n",
    "â”‚    â””â”€Linear: 2-9                       [16, 10]                  1,970\n",
    "==========================================================================================\n",
    "Total params: 2,174\n",
    "Trainable params: 2,174\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 1.00\n",
    "==========================================================================================\n",
    "Input size (MB): 0.05\n",
    "Forward/backward pass size (MB): 1.00\n",
    "Params size (MB): 0.01\n",
    "Estimated Total Size (MB): 1.06\n",
    "==========================================================================================\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84b79b5f",
   "metadata": {},
   "source": [
    "## Boost scikit-learns performance with Intel Extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43c15ee6",
   "metadata": {},
   "source": [
    "Scikit-learn is one of the most popular ML packages for Python.\n",
    "\n",
    "But, to be honest, their algorithms are not the fastest ones.\n",
    "\n",
    "With Intelâ€™s Extension for scikit-learn, `scikit-learn-intelex`. you can speed up training time for some favourite algorithms like:\n",
    "\n",
    "- Support Vector Classifier/Regressor\n",
    "- Random Forest Classifier/Regressor\n",
    "- LASSO\n",
    "- DBSCAN\n",
    "\n",
    "Just add two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(\n",
    "n_samples=100000, \n",
    "n_features=10, \n",
    "noise=0.5)\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45df4722",
   "metadata": {},
   "source": [
    "## Incorportate Domain Knowledge into XGBoost with Feature Interaction Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c910a4",
   "metadata": {},
   "source": [
    "Want to incorporate your domain knowledge into `ğ—ğ†ğğ¨ğ¨ğ¬ğ­`?\n",
    "\n",
    "Try using ğ…ğğšğ­ğ®ğ«ğ ğˆğ§ğ­ğğ«ğšğœğ­ğ¢ğ¨ğ§ ğ‚ğ¨ğ§ğ¬ğ­ğ«ğšğ¢ğ§ğ­ğ¬.\n",
    "\n",
    "Feature Interaction Constraints allow you to control which features are allowed to interact with each other and which are not while building the trees.\n",
    "\n",
    "For example, the constraint [0, 1] means that Feature_0 and Feature_1 are allowed to interact with each other but with no other variable. Similarly, [3, 5, 9] means that Feature_3, Feature_5, and Feature_9 are allowed to interact with each other but with no other variable.\n",
    "\n",
    "With this in mind, you can define feature interaction constraints:\n",
    "\n",
    "- Based on domain knowledge, when you know that some features interactions will lead to better results\n",
    "- Based on regulatory constraints in your industry/company where some features can not interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X, y = ...\n",
    "\n",
    "dmatrix = xgb.DMatrix(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"interaction_constraints\": [[0,2 ], [1, 3, 4]]\n",
    "}\n",
    "\n",
    "model_with_constraints = xgb.train(params, dmatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7de62e4",
   "metadata": {},
   "source": [
    "## Powerful AutoML with FLAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518ab3e",
   "metadata": {},
   "source": [
    "Do you always hear about AutoML?\n",
    "\n",
    "And want to try it out?\n",
    "\n",
    "Use `FLAML` for Python.\n",
    "\n",
    "`FLAML` (Fast and Lightweight AutoML) is an AutoML package developed by Microsoft.\n",
    "\n",
    "It can do Model Selection, Hyperparameter tuning, and Feature Engineering automatically.\n",
    "\n",
    "Thus, it removes the pain of choosing the best model and parameters so that you can focus more on your data.\n",
    "\n",
    "Per default, its estimator list contains only tree-based models like XGBoost, CatBoost, and LightGBM. But you can also add custom models.\n",
    "\n",
    "A powerful library!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd994b6f",
   "metadata": {},
   "source": [
    "## Aspect-based Seniment Analysis with PyABSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbc76a7a",
   "metadata": {},
   "source": [
    "Traditional sentiment analysis focuses on determining the overall sentiment of a piece of text.\n",
    "\n",
    "For example, the sentence :\n",
    "\n",
    "â€œThe food was bad and the staff was rudeâ€\n",
    "\n",
    "would output only a negative sentiment.\n",
    "\n",
    "But, what if I want to extract, which aspects have a negative or positive sentiment?\n",
    "\n",
    "Thatâ€™s the responsibility of aspect-based sentiment analysis.\n",
    "\n",
    "It aims to identify and extract the sentiment expressed towards specific aspects of a text.\n",
    "\n",
    "For the sentence:\n",
    "\n",
    "â€The battery life is excellent but the camera quality is bad.â€\n",
    "\n",
    "a model's output would be:\n",
    "\n",
    "- Battery life: positive\n",
    "- Camera quality: negative\n",
    "\n",
    "With aspect-based sentiment analysis, you can understand the opinions and feelings expressed about specific aspects.\n",
    "\n",
    "To do that in Python, use the package `PyABSA`.\n",
    "\n",
    "It contains pre-trained models with an easy-to-use API for aspect-term extraction and sentiment classification.\n",
    "\n",
    "`PyABSA` can be used for a variety of applications, such as:\n",
    "\n",
    "- Customer feedback analysis\n",
    "- Product reviews analysis\n",
    "- Social media monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyabsa==1.16.27\n",
    "\n",
    "from pyabsa import ATEPCCheckpointManager\n",
    "\n",
    "extractor = ATEPCCheckpointManager.get_aspect_extractor(\n",
    "                  checkpoint=\"multilingual\",\n",
    "                  auto_device=False\n",
    ")\n",
    "                                                        \n",
    "example = [\"Location and food were excellent but stuff was very unfriendly.\"]\n",
    "result = extractor.extract_aspect(inference_source=example, pred_sentiment=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d8f01a9",
   "metadata": {},
   "source": [
    "## Use XGBoost for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "xgbrf = XGBRFRegressor(n_estimators=100)\n",
    "\n",
    "X = np.random.rand(100000, 10)\n",
    "y = np.random.rand(100000)\n",
    "\n",
    "xgbrf.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "54bf0f8d5625db32e314b5bdaf50a44046044c99ae376da8e1ac5bc25f06b01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
