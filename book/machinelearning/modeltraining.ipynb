{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6603ef12",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "617e059c",
   "metadata": {},
   "source": [
    "## Compute Class Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "667a14ad",
   "metadata": {},
   "source": [
    "To handle class imbalance in Machine Learning, there are several methods.\n",
    "<br><br>\n",
    "One of them is adjusting the class weights. \n",
    "<br><br>\n",
    "By giving higher weights to the minority class and lower weights to the majority class, we can regularize the loss function.\n",
    "<br><br>\n",
    "Misclassifying the minority class will result in a higher loss due to the higher weight.\n",
    "<br><br>\n",
    "To incorporate class weights in Tensorflow, use `scikit-learn`'s `compute_class_weight` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfff72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "X, y = ...\n",
    "\n",
    "# will return an array with weights for each class, e.g. [0.6, 0.6, 1.]\n",
    "class_weights = compute_class_weight(\n",
    "  class_weight=\"balanced\",\n",
    "  classes=np.unique(y),\n",
    "  y=y\n",
    ")\n",
    "\n",
    "# to get a dictionary with {<class>:<weight>}\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = tf.keras.Sequential(...)\n",
    "model.compile(...)\n",
    "\n",
    "# using class_weights in the .fit() method\n",
    "model.fit(X, y, class_weight=class_weights, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1981d413",
   "metadata": {},
   "source": [
    "## Reset TensorFlow/Keras Global State"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8279453",
   "metadata": {},
   "source": [
    "In Tensorflow/Keras, when you create multiple models in a loop, you will need `tf.keras.backend.clear_session()`.\n",
    "\n",
    "Keras manages a global state, which includes configurations and the current values (weights and biases) of the models.\n",
    "\n",
    "So when you create a model in a loop, the global state gets bigger and bigger with every created model. To clear the state, 𝐝𝐞𝐥 𝐦𝐨𝐝𝐞𝐥 will not work because it will only delete the Python variable.\n",
    "\n",
    "So `tf.keras.backend.clear_session()` is a better option. It will reset the state of a model and helps avoid clutter from old models.\n",
    "\n",
    "See the first example below. Each iteration of this loop will increase the size of the global state and of your memory.\n",
    "\n",
    "In the second example, the memory consumption stays constant by clearing the state with every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model():\n",
    "  model = tf.keras.Sequential(...)\n",
    "  return model\n",
    "\n",
    "# without clearing session\n",
    "for _ in range(20):\n",
    "  model = create_model()\n",
    "  \n",
    "# with clearing session\n",
    "for _ in range(20):\n",
    "  tf.keras.backend.clear_session()\n",
    "  model = create_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bd2797b",
   "metadata": {},
   "source": [
    "## Find dirty labels with `cleanlab`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc71d9b",
   "metadata": {},
   "source": [
    "Do you want to identify noisy labels in your dataset?\n",
    "\n",
    "Try `cleanlab` for Python.\n",
    "\n",
    "`cleanlab` is a data-centric AI package to automatically detect noisy labels and address dataset issues to fix them via confident learning algorithms.\n",
    "\n",
    "It works with nearly every model possible:\n",
    "\n",
    "- XGBoost\n",
    "- scikit-learn models\n",
    "- Tensorflow\n",
    "- PyTorch\n",
    "- HuggingFace\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "cl = cleanlab.classification.CleanLearning(clf)\n",
    "\n",
    "label_issues = cl.find_label_issues(X, y)\n",
    "\n",
    "print(label_issues.query('is_label_issue == True'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec0393c",
   "metadata": {},
   "source": [
    "## Evaluate your Classifier with sklearn's `classification_report`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61abaf8c",
   "metadata": {},
   "source": [
    "Would you like to evaluate your Machine Learning model quickly?\n",
    "\n",
    "Try `classification_report` from scikit-learn\n",
    "\n",
    "With `classification_report`, you can quickly assess the performance of your model.\n",
    "\n",
    "It summarizes Precision, Recall, F1-Score, and Support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd943215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small script where sklearns classification_report is used\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "     class 0       0.50      1.00      0.67         1\n",
    "     class 1       0.00      0.00      0.00         1\n",
    "     class 2       1.00      0.67      0.80         3\n",
    "\n",
    "    accuracy                           0.60         5\n",
    "   macro avg       0.50      0.56      0.49         5\n",
    "weighted avg       0.70      0.60      0.61         5\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c43a832",
   "metadata": {},
   "source": [
    "## Obtain Reproducible Optimizations Results in Optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206adcb1",
   "metadata": {},
   "source": [
    "Optuna is a powerful hyperparameter optimization framework that supports many machine learning frameworks, including TensorFlow, PyTorch, and XGBoost.\n",
    "\n",
    "But you need to be careful with reproducible results for hyperparameter tuning.tuple\n",
    "\n",
    "To achieve reproducible results, you need to set the seed for your Sampler.\n",
    "\n",
    "Below you can see how it is done for `TPESampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective(trial):\n",
    "    ...\n",
    "    \n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ed7523",
   "metadata": {},
   "source": [
    "## Find bad labels with `doubtlab`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99e1ddc1",
   "metadata": {},
   "source": [
    "Do you want to find bad labels in your data?\n",
    "\n",
    "Try `doubtlab` for Python.\n",
    "\n",
    "With `doubtlab`, you can define reasons to doubt your labels and take a closer look.\n",
    "\n",
    "Reasons to doubt your labels can be for example:\n",
    "\n",
    "- 𝐏𝐫𝐨𝐛𝐚𝐑𝐞𝐚𝐬𝐨𝐧: When the confidence values are low for any label\n",
    "- 𝐖𝐫𝐨𝐧𝐠𝐏𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐑𝐞𝐚𝐬𝐨𝐧: When a model cannot predict the listed label\n",
    "- 𝐃𝐢𝐬𝐚𝐠𝐫𝐞𝐞𝐑𝐞𝐚𝐬𝐨𝐧: When two models disagree on a prediction.\n",
    "- 𝐑𝐞𝐥𝐚𝐭𝐢𝐯𝐞𝐃𝐢𝐟𝐟𝐞𝐫𝐞𝐧𝐜𝐞𝐑𝐞𝐚𝐬𝐨𝐧: When the relative difference between label and prediction is too high\n",
    "\n",
    "So, identify your noisy labels and fix them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ddaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install doubtlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubtlab.ensemble import DoubtEnsemble\n",
    "from doubtlab.reason import ProbaReason, WrongPredictionReason\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Define reasons to check\n",
    "reasons = {\n",
    "    'proba': ProbaReason(model=model),\n",
    "    'wrong_pred': WrongPredictionReason(model=model),\n",
    "}\n",
    "\n",
    "# Pass reasons to DoubtLab instance\n",
    "doubt = DoubtEnsemble(**reasons)\n",
    "\n",
    "# Returns DataFrame with reasoning\n",
    "predicates = doubt.get_predicates(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44919184",
   "metadata": {},
   "source": [
    "## Get notified when your model is finished with training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c998cfa5",
   "metadata": {},
   "source": [
    "Never stare at your screen, waiting for your model to finish training.\n",
    "\n",
    "Try `knockknock` for Python.\n",
    "\n",
    "`knockknock` is a library that notifies you when your training is finished.\n",
    "\n",
    "You only need to add a decorator.\n",
    "\n",
    "Currently, you can get a notification through 12 different channels\n",
    "like:\n",
    "\n",
    "- Email\n",
    "- Slack\n",
    "- Telegram\n",
    "- Discord\n",
    "- MS Teams\n",
    "\n",
    "\n",
    "Use it for your future model training and don’t stick to your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install knockknock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from knockknock import email_sender\n",
    "\n",
    "@email_sender(recipient_emails=[\"coolmail@python.com\", \"2coolmail@python.com\"], sender_email=\"anothercoolmail@python.com\")\n",
    "def train_model(model, X, y):\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c01896ba",
   "metadata": {},
   "source": [
    "## Get Model Summary in PyTorch with `torchinfo`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd63bee6",
   "metadata": {},
   "source": [
    "Do you want a Model summary in PyTorch?\n",
    "\n",
    "Like in Keras with `model.summary()`?\n",
    "\n",
    "Use `torchinfo`.\n",
    "\n",
    "With `torchinfo`, you can get a model summary as you know it from\n",
    "Keras.\n",
    "\n",
    "Just add one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "class MyModel(torch.nn.Module)\n",
    "  ...\n",
    "  \n",
    "model = MyModel()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "summary(model, input_size=(BATCH_SIZE, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "Net                                      [16, 10]                  --\n",
    "├─Sequential: 1-1                        [16, 4, 7, 7]             --\n",
    "│    └─Conv2d: 2-1                       [16, 4, 28, 28]           40\n",
    "│    └─BatchNorm2d: 2-2                  [16, 4, 28, 28]           8\n",
    "│    └─ReLU: 2-3                         [16, 4, 28, 28]           --\n",
    "│    └─MaxPool2d: 2-4                    [16, 4, 14, 14]           --\n",
    "│    └─Conv2d: 2-5                       [16, 4, 14, 14]           148\n",
    "│    └─BatchNorm2d: 2-6                  [16, 4, 14, 14]           8\n",
    "│    └─ReLU: 2-7                         [16, 4, 14, 14]           --\n",
    "│    └─MaxPool2d: 2-8                    [16, 4, 7, 7]             --\n",
    "├─Sequential: 1-2                        [16, 10]                  --\n",
    "│    └─Linear: 2-9                       [16, 10]                  1,970\n",
    "==========================================================================================\n",
    "Total params: 2,174\n",
    "Trainable params: 2,174\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 1.00\n",
    "==========================================================================================\n",
    "Input size (MB): 0.05\n",
    "Forward/backward pass size (MB): 1.00\n",
    "Params size (MB): 0.01\n",
    "Estimated Total Size (MB): 1.06\n",
    "==========================================================================================\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84b79b5f",
   "metadata": {},
   "source": [
    "## Boost scikit-learns performance with Intel Extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43c15ee6",
   "metadata": {},
   "source": [
    "Scikit-learn is one of the most popular ML packages for Python.\n",
    "\n",
    "But, to be honest, their algorithms are not the fastest ones.\n",
    "\n",
    "With Intel’s Extension for scikit-learn, `scikit-learn-intelex`. you can speed up training time for some favourite algorithms like:\n",
    "\n",
    "- Support Vector Classifier/Regressor\n",
    "- Random Forest Classifier/Regressor\n",
    "- LASSO\n",
    "- DBSCAN\n",
    "\n",
    "Just add two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(\n",
    "n_samples=100000, \n",
    "n_features=10, \n",
    "noise=0.5)\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45df4722",
   "metadata": {},
   "source": [
    "## Incorportate Domain Knowledge into XGBoost with Feature Interaction Constraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7c910a4",
   "metadata": {},
   "source": [
    "Want to incorporate your domain knowledge into `𝐗𝐆𝐁𝐨𝐨𝐬𝐭`?\n",
    "\n",
    "Try using 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐈𝐧𝐭𝐞𝐫𝐚𝐜𝐭𝐢𝐨𝐧 𝐂𝐨𝐧𝐬𝐭𝐫𝐚𝐢𝐧𝐭𝐬.\n",
    "\n",
    "Feature Interaction Constraints allow you to control which features are allowed to interact with each other and which are not while building the trees.\n",
    "\n",
    "For example, the constraint [0, 1] means that Feature_0 and Feature_1 are allowed to interact with each other but with no other variable. Similarly, [3, 5, 9] means that Feature_3, Feature_5, and Feature_9 are allowed to interact with each other but with no other variable.\n",
    "\n",
    "With this in mind, you can define feature interaction constraints:\n",
    "\n",
    "- Based on domain knowledge, when you know that some features interactions will lead to better results\n",
    "- Based on regulatory constraints in your industry/company where some features can not interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X, y = ...\n",
    "\n",
    "dmatrix = xgb.DMatrix(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"interaction_constraints\": [[0,2 ], [1, 3, 4]]\n",
    "}\n",
    "\n",
    "model_with_constraints = xgb.train(params, dmatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7de62e4",
   "metadata": {},
   "source": [
    "## Powerful AutoML with `FLAML`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d518ab3e",
   "metadata": {},
   "source": [
    "Do you always hear about AutoML?\n",
    "\n",
    "And want to try it out?\n",
    "\n",
    "Use `FLAML` for Python.\n",
    "\n",
    "`FLAML` (Fast and Lightweight AutoML) is an AutoML package developed by Microsoft.\n",
    "\n",
    "It can do Model Selection, Hyperparameter tuning, and Feature Engineering automatically.\n",
    "\n",
    "Thus, it removes the pain of choosing the best model and parameters so that you can focus more on your data.\n",
    "\n",
    "Per default, its estimator list contains only tree-based models like XGBoost, CatBoost, and LightGBM. But you can also add custom models.\n",
    "\n",
    "A powerful library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e91cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train, task=\"classification\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd994b6f",
   "metadata": {},
   "source": [
    "## Aspect-based Seniment Analysis with `PyABSA`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbc76a7a",
   "metadata": {},
   "source": [
    "Traditional sentiment analysis focuses on determining the overall sentiment of a piece of text.\n",
    "\n",
    "For example, the sentence :\n",
    "\n",
    "“The food was bad and the staff was rude”\n",
    "\n",
    "would output only a negative sentiment.\n",
    "\n",
    "But, what if I want to extract, which aspects have a negative or positive sentiment?\n",
    "\n",
    "That’s the responsibility of aspect-based sentiment analysis.\n",
    "\n",
    "It aims to identify and extract the sentiment expressed towards specific aspects of a text.\n",
    "\n",
    "For the sentence:\n",
    "\n",
    "”The battery life is excellent but the camera quality is bad.”\n",
    "\n",
    "a model's output would be:\n",
    "\n",
    "- Battery life: positive\n",
    "- Camera quality: negative\n",
    "\n",
    "With aspect-based sentiment analysis, you can understand the opinions and feelings expressed about specific aspects.\n",
    "\n",
    "To do that in Python, use the package `PyABSA`.\n",
    "\n",
    "It contains pre-trained models with an easy-to-use API for aspect-term extraction and sentiment classification.\n",
    "\n",
    "`PyABSA` can be used for a variety of applications, such as:\n",
    "\n",
    "- Customer feedback analysis\n",
    "- Product reviews analysis\n",
    "- Social media monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyabsa==1.16.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa import ATEPCCheckpointManager\n",
    "\n",
    "extractor = ATEPCCheckpointManager.get_aspect_extractor(\n",
    "                  checkpoint=\"multilingual\",\n",
    "                  auto_device=False\n",
    ")\n",
    "                                                        \n",
    "example = [\"Location and food were excellent but stuff was very unfriendly.\"]\n",
    "result = extractor.extract_aspect(inference_source=example, pred_sentiment=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d8f01a9",
   "metadata": {},
   "source": [
    "## Use XGBoost for Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffed2c5",
   "metadata": {},
   "source": [
    "Are you still using Random Forests from sklearn?\n",
    "\n",
    "XGBoost implements Random Forests too, and much faster than sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "xgbrf = XGBRFRegressor(n_estimators=100)\n",
    "\n",
    "X = np.random.rand(100000, 10)\n",
    "y = np.random.rand(100000)\n",
    "\n",
    "xgbrf.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d210ee",
   "metadata": {},
   "source": [
    "## Identify problematic images with `cleanvision`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3946c3ae",
   "metadata": {},
   "source": [
    "Your Deep Learning Model doesn’t perform?\n",
    "\n",
    "It’s probably because of your data.\n",
    "\n",
    "With `cleanvision`, you can detect issues in image data.\n",
    "\n",
    "`cleanvision` is a relatively new data-centric AI package to find problems in your image dataset.\n",
    "\n",
    "It can detect issues like:\n",
    "\n",
    "- Exact or Near Duplicates\n",
    "- Blurry Images\n",
    "- Odd Aspect Ratios\n",
    "- Irregularly Dark/Light images\n",
    "- Images lacking content\n",
    "\n",
    "A good first step to try before applying crazy Vision Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget - nc 'https://cleanlab-public.s3.amazonaws.com/CleanVision/image_files.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q image_files.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cleanvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f056537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanvision.imagelab import Imagelab\n",
    "\n",
    "# Path to your dataset, you can specify your own dataset path\n",
    "dataset_path = \"./image_files/\"\n",
    "\n",
    "# Initialize imagelab with your dataset\n",
    "imagelab = Imagelab(data_path=dataset_path)\n",
    "\n",
    "# Find issues\n",
    "imagelab.find_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of issues with prevalence per issue\n",
    "imagelab.issue_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Top examples for blurry images\n",
    "imagelab.visualize(issue_types=['blurry'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ade41e86",
   "metadata": {},
   "source": [
    "## Select the optimal Regularization Parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95d62fa9",
   "metadata": {},
   "source": [
    "How do you choose your Regularization Parameter?\n",
    "\n",
    "Your model's complexity decreases with a higher Regularization Parameter (Alpha).\n",
    "\n",
    "It shouldn’t be too high or too low.\n",
    "\n",
    "Yellowbrick’s `𝐀𝐥𝐩𝐡𝐚𝐒𝐞𝐥𝐞𝐜𝐭𝐢𝐨𝐧` can help you to find the best Alpha.\n",
    "\n",
    "It takes your model and visualizes the Alpha/Error curve so you can see how the model’s error responds to different alpha values.\n",
    "\n",
    "Below you can see how to do it with scikit-learn’s LassoCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Create a list of alphas to cross-validate against\n",
    "alphas = np.linspace(0, 10, 30)\n",
    "\n",
    "model = LassoCV(alphas=alphas)\n",
    "visualizer = AlphaSelection(model)\n",
    "visualizer.fit(X, y)\n",
    "visualizer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e0fe145",
   "metadata": {},
   "source": [
    "## Decision Forests in TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8ca5763",
   "metadata": {},
   "source": [
    "Did you know there are Decision Forests from TensorFlow?\n",
    "\n",
    "`tensorflow_decision_forests` implements decision forest models like Random Forest or GBDT for classification, regression, and ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_decision_forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = tf.keras.utils.get_file(\n",
    "      \"adult.csv\",\n",
    "      \"https://raw.githubusercontent.com/google/yggdrasil-decision-forests/\"\n",
    "      \"main/yggdrasil_decision_forests/test_data/dataset/adult.csv\")\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_path)\n",
    "test_indices = np.random.rand(len(dataset_df)) < 0.30\n",
    "test_ds_pd = dataset_df[test_indices]\n",
    "train_ds_pd = dataset_df[~test_indices]\n",
    "\n",
    "\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"income\")\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=\"income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a174492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(verbose=2)\n",
    "model.fit(train_ds)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "830545e3",
   "metadata": {},
   "source": [
    "## AutoML with `AutoGluon`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33a3a4a3",
   "metadata": {},
   "source": [
    "Do you always hear about AutoML?\n",
    "\n",
    "And want to try it out?\n",
    "\n",
    "Use `AutoGluon` for Python.\n",
    "\n",
    "`AutoGluon` is a Python package from AWS.\n",
    "\n",
    "It lets you perform AutoML on:\n",
    "\n",
    "- Tabular Data (Classification, Regression)\n",
    "- Time Series Data\n",
    "- Multimodal Data (Images + Text + Tabular)\n",
    "\n",
    "Thus, it removes the pain of choosing the best model and best parameter.\n",
    "\n",
    "AutoGluon also offers utilities for EDA, like:\n",
    "\n",
    "- Detecting Covariate Shift\n",
    "- Target Variable Analysis\n",
    "- Feature Interaction Charts\n",
    "\n",
    "See below for a quickstart for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd347e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34468eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='class').fit(train_data, time_limit=240)\n",
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fd9457d",
   "metadata": {},
   "source": [
    "## Visualize Keras Models with `visualkeras`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8d28cd1",
   "metadata": {},
   "source": [
    "Do you want some cool visualization for your Deep Learning Models?\n",
    "\n",
    "Try `visualkeras`.\n",
    "\n",
    "`visualkeras` visualizes your Keras models (as an alternative to model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, to_file='output.png').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b786a6d9",
   "metadata": {},
   "source": [
    "## Perform Multilabel Stratified KFold with `iterative-stratification`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89bdf863",
   "metadata": {},
   "source": [
    "When doing Cross-Validation for classification,\n",
    "\n",
    "StratifiedKFold from scikit-learn is a common choice.\n",
    "\n",
    "Stratification aims to guarantee that every fold represents all strata of the data.\n",
    "\n",
    "But, scikit-learn doesn't support stratifying multilabel data.\n",
    "\n",
    "For this use case, try the `iterative-stratification` package.\n",
    "\n",
    "It offers implementations for stratyfing multilabel data in different ways.\n",
    "\n",
    "See below how we can use MultilabelStratifiedKFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6619962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, test_index in mskf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e963f",
   "metadata": {},
   "source": [
    "## Interpret your Model with `Shapash`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c57166",
   "metadata": {},
   "source": [
    "Nobody cares about your SOTA ML Model if nobody can’t understand the predictions.\n",
    "\n",
    "Therefore, interpretability of ML Models is a crucial point in industry cases.\n",
    "\n",
    "To overcome this hurdle, use `Shapash` for Python.\n",
    "\n",
    "`Shapash` offers several types of interpretability methods to understand your model’s predictions like:\n",
    "\n",
    "- Feature Importance\n",
    "- Feature Contribution\n",
    "- LIME\n",
    "- SHAP\n",
    "\n",
    "It comes also with an intuitive GUI to interact with.\n",
    "\n",
    "Check it out! Link is in the comments section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shapash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from category_encoders import OrdinalEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from shapash.data.data_loader import data_loading\n",
    "house_df, house_dict = data_loading('house_prices')\n",
    "\n",
    "y_df=house_df['SalePrice'].to_frame()\n",
    "X_df=house_df[house_df.columns.difference(['SalePrice'])]\n",
    "\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "categorical_features = [col for col in X_df.columns if X_df[col].dtype == 'object']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    cols=categorical_features,\n",
    "    handle_unknown='ignore',\n",
    "    return_df=True).fit(X_df)\n",
    "\n",
    "X_df=encoder.transform(X_df)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_df, y_df, train_size=0.75, random_state=1)\n",
    "\n",
    "regressor = LGBMRegressor(n_estimators=100).fit(Xtrain,ytrain)\n",
    "\n",
    "from shapash import SmartExplainer\n",
    "\n",
    "xpl = SmartExplainer(\n",
    "    model=regressor,\n",
    "    preprocessing=encoder,  \n",
    "    features_dict=house_dict  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpl.compile(x=Xtest,\n",
    "            y_target=ytest \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = xpl.run_app(title_story='House Prices', port=8020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d6075",
   "metadata": {},
   "source": [
    "## Validate Your Model and Data with `Deepchecks`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9450c",
   "metadata": {},
   "source": [
    "Validating your Model and Data is crucial in ML.\n",
    "\n",
    "Not testing them will cause huge problems in production.\n",
    "\n",
    "To change that, use `deepchecks`.\n",
    "\n",
    "`deepchecks` is an open-source solution which offers a suite for detailed validation methods.\n",
    "\n",
    "It will calculate and visualize a bunch of things like:\n",
    "\n",
    "- Train/Test Performance\n",
    "- Predictive Power Score\n",
    "- Feature Drift\n",
    "- Label Drift\n",
    "- Weak Segments for your model\n",
    "\n",
    "A powerful tool to consider for testing your models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepchecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360011cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deepchecks.tabular.datasets.classification import iris\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.suites import full_suite\n",
    "\n",
    "# Load Data\n",
    "iris_df = iris.load_data(data_format='Dataframe', as_train_test=False)\n",
    "label_col = 'target'\n",
    "df_train, df_test = train_test_split(iris_df, stratify=iris_df[label_col], random_state=0)\n",
    "\n",
    "# Train Model\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "rf_clf.fit(df_train.drop(label_col, axis=1), df_train[label_col])\n",
    "\n",
    "\n",
    "ds_train = Dataset(df_train, label=label_col, cat_features=[])\n",
    "ds_test =  Dataset(df_test,  label=label_col, cat_features=[])\n",
    "\n",
    "suite = full_suite()\n",
    "\n",
    "suite.run(train_dataset=ds_train, test_dataset=ds_test, model=rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b6ea8",
   "metadata": {},
   "source": [
    "## Visualize high-performance Features with `Optuna`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab01a8",
   "metadata": {},
   "source": [
    "Optuna released a new feature for detecting high-performing parameters.\n",
    "\n",
    "Its `plot_rank()` function visualizes different parameters, with individual points representing individual trials.\n",
    "\n",
    "Since the plot is interactive, you can also hover over it and dive deeper into analysing your hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=trial.suggest_int('Mdpth', 2, 32, log=True),\n",
    "        min_samples_split=trial.suggest_int('mspl', 2, 32, log=True),\n",
    "        min_samples_leaf=trial.suggest_int('mlfs', 1, 32, log=True),\n",
    "        min_weight_fraction_leaf=trial.suggest_float('mwfr', 0.0, 0.5),\n",
    "        max_features=trial.suggest_int(\"Mfts\", 1, 15),\n",
    "        max_leaf_nodes=trial.suggest_int('Mnods', 4, 100, log=True),\n",
    "        min_impurity_decrease=trial.suggest_float('mid', 0.0, 0.5),\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)\n",
    "\n",
    "# Optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get parameters sorted by the importance values\n",
    "importances = optuna.importance.get_param_importances(study)\n",
    "params_sorted = list(importances.keys())\n",
    "\n",
    "# Plot\n",
    "fig = optuna.visualization.plot_rank(study, params=params_sorted[:4])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28220dc8",
   "metadata": {},
   "source": [
    "## Model Ensembling with `combo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcc2d3",
   "metadata": {},
   "source": [
    "Looking at the top solutions on Kaggle you will notice one thing:\n",
    "\n",
    "There is usually some sort of combination of various ML models involved.\n",
    "\n",
    "With `combo` for Python, you can combine \n",
    "\n",
    "- Multiple Classifiers\n",
    "- Multiple Anomaly Detection Models\n",
    "- Multiple Clustering Models\n",
    "\n",
    "`combo` also offers multiple combination methods for every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from combo.models.cluster_comb import ClustererEnsemble\n",
    "\n",
    "estimators = [KMeans(n_clusters=n_clusters),\n",
    "              MiniBatchKMeans(n_clusters=n_clusters),\n",
    "              AgglomerativeClustering(n_clusters=n_clusters)]\n",
    "\n",
    "clf = ClustererEnsemble(estimators, n_clusters=n_clusters)\n",
    "clf.fit(X)\n",
    "\n",
    "aligned_labels = clf.aligned_labels_\n",
    "predicted_labels = clf.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bd150",
   "metadata": {},
   "source": [
    "## Residual Plots with `yellowbrick`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16ade1",
   "metadata": {},
   "source": [
    "To analyze the variance of the error of your Regression model\n",
    "\n",
    "\n",
    "Use `ResidualPlot` from `yellowbrick`.\n",
    "\n",
    "\n",
    "With Residual Plots, you can see how well-fitted your model is.\n",
    "\n",
    "\n",
    "If the data points exhibit a random distribution along the horizontal axis, a linear regression model is typically suitable, whereas in cases of non-random dispersion, a non-linear model is a better choice. \n",
    "\n",
    "\n",
    "See below how you can easily implement that with `yellowbrick`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "model = Lasso()\n",
    "visualizer = ResidualsPlot(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d30f01",
   "metadata": {},
   "source": [
    "## Powerful and Distributed Hyperparameter Optimization with `ray.tune`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5824a",
   "metadata": {},
   "source": [
    "Do you need hyperparameter tuning on steroids?\n",
    "\n",
    "Try `tune` from `ray`.\n",
    "\n",
    "`tune` performs distributed hyperparameter tuning with multi-GPU and multi-node support, utilizing all the hardware you have.\n",
    "\n",
    "It supports the most popular ML libraries and integrates many other common hyperparameter optimization tools like Optuna or Hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"ray[tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fa787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"ray[tune]\"\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import xgboost as xgb\n",
    "from ray import train, tune\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_breast_cancer(config):\n",
    "    data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.25)\n",
    "    train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "    test_set = xgb.DMatrix(test_x, label=test_y)\n",
    "    results = {}\n",
    "    xgb.train(\n",
    "        config,\n",
    "        train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=results,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    accuracy = 1.0 - results[\"eval\"][\"error\"][-1]\n",
    "    train.report({\"mean_accuracy\": accuracy, \"done\": True})\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_breast_cancer,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=10,\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"mean_accuracy\", mode=\"max\").config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62334c73",
   "metadata": {},
   "source": [
    "## Use PyTorch with scikit-learn API with `skorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac45c78",
   "metadata": {},
   "source": [
    "PyTorch and scikit-learn are one of the most popular libraries for ML/DL.\n",
    "\n",
    "So, why not combine PyTorch with scikit-learn?\n",
    "\n",
    "Try `skorch`!\n",
    "\n",
    "`skorch` is a high-level library for PyTorch that provides a scikit-learn-compatible neural network module.\n",
    "\n",
    "It allows you to use the simple scikit-learn interface for PyTorch.\n",
    "\n",
    "Therefore you can integrate PyTorch models into scikit-learn workflows.\n",
    "\n",
    "See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a00fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ed770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('net', net),\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)\n",
    "y_proba = pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead6256",
   "metadata": {},
   "source": [
    "## Online ML with `river`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63f8ef",
   "metadata": {},
   "source": [
    "Do you want ML models that learn on-the-fly from massive datasets?\n",
    "\n",
    "Try `river`.\n",
    "\n",
    "`river` is a library for online machine learning.\n",
    "\n",
    "You can continuously update your model with streaming data without using the full dataset for training again.\n",
    "\n",
    "It provides online implementations for many algorithms like KNN, Tree-based models and Recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "for x, y in dataset:\n",
    "    y_pred = model.predict_one(x)     \n",
    "    metric.update(y, y_pred)           \n",
    "    model.learn_one(x, y)              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a9aec",
   "metadata": {},
   "source": [
    "## SOTA Computer Vision Models with `timm` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6258d",
   "metadata": {},
   "source": [
    "Do you want to use SOTA computer vision models?\n",
    "\n",
    "Try `timm`.\n",
    "\n",
    "`timm` (PyTorch Image Models) is a library which contains multiple computer vision models, layers, optimizers, etc. \n",
    "\n",
    "It provides models like Vision Transformer, MobileNet, Swin Transformer, ConvNeXt, DenseNet, and more.\n",
    "\n",
    "You just have to define the name of the model and if you want to have the pretrained weights of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2009724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45167f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "print(timm.list_models())\n",
    "\n",
    "model = timm.create_model('densenet121', pretrained=True)\n",
    "output = model(torch.randn(2, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69feb793",
   "metadata": {},
   "source": [
    "## Generate Guaranteed Prediction Intervals and Sets with `MAPIE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41226b58",
   "metadata": {},
   "source": [
    "For quantifying uncertainties of your models, use MAPIE.\n",
    "\n",
    "`MAPIE` (Model Agnostic Prediction Interval Estimator) takes your sklearn-/tensorflow-/pytorch-compatible model and generate prediction intervals or sets with guaranteed coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mapie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapie.regression import MapieRegressor\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=1, noise=20, random_state=59)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "regressor = LinearRegression()\n",
    "\n",
    "mapie_regressor = MapieRegressor(regressor)\n",
    "mapie_regressor.fit(X_train, y_train)\n",
    "\n",
    "alpha = [0.05, 0.20]\n",
    "y_pred, y_pis = mapie_regressor.predict(X_test, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f6fab",
   "metadata": {},
   "source": [
    "## Extra Components For scikit-learn with `scikit-lego`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e041d4",
   "metadata": {},
   "source": [
    "scikit-learn is one of the most popular ML libraries.\n",
    "\n",
    "While it's easy to write custom components, it would be nice to have all of them in a single place.\n",
    "\n",
    "`scikit-lego` is such a library which contains many custom components like:\n",
    "\n",
    "- `DebugPipeline`, which adds debug information to pipelines\n",
    "- `ImbalancedLinearRegression` to punish over-/underestimation of a model\n",
    "- `add_lags` to add lag values to a DataFrame\n",
    "- `ZeroInflatedRegressor` which predicts zero or applies a regression based on a classifier\n",
    "\n",
    "and many more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-lego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c86d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklego.preprocessing import RandomAdder\n",
    "from sklego.mixture import GMMClassifier\n",
    "\n",
    "...\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"random_noise\", RandomAdder()),\n",
    "    (\"model\", GMMClassifier())\n",
    "])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94badc36",
   "metadata": {},
   "source": [
    "## Quantize your Models with `torchao`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f98cc3",
   "metadata": {},
   "source": [
    "Quantizing your Deep Learning models was never easier.\n",
    "\n",
    "With `torchao`, you can quantize and sparsify your models with 1 line of code.\n",
    "\n",
    "If you are unsure which method to use, you can even use the autoquant method to quantize your layers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b50502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchao\n",
    "\n",
    "model = torchao.autoquant(torch.compile(model, mode='max-autotune'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c127a42",
   "metadata": {},
   "source": [
    "## Conformal Prediction with `Conformal Tights`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea5a2c",
   "metadata": {},
   "source": [
    "Create valid prediction intervals with **Conformal Tights** in Python!\n",
    "\n",
    "**Conformal Tights** is a Python library which makes it easy to use Conformal Prediction for every scikit-learn regressor or Darts forecasting model.\n",
    "\n",
    "With a few lines of code, you will get accurate prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conformal-tights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conformal_tights import ConformalCoherentQuantileRegressor\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "X, y = fetch_openml(\"ames_housing\", version=1, return_X_y=True, as_frame=True, parser=\"auto\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "my_regressor = XGBRegressor(objective=\"reg:absoluteerror\")\n",
    "conformal_predictor = ConformalCoherentQuantileRegressor(estimator=my_regressor)\n",
    "conformal_predictor.fit(X_train, y_train)\n",
    "\n",
    "y_test = conformal_predictor.predict(X_test)\n",
    "\n",
    "y_test_quantiles = conformal_predictor.predict_quantiles(\n",
    "    X_test, quantiles=(0.05, 0.1, 0.5, 0.9, 0.95)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8818be",
   "metadata": {},
   "source": [
    "## Fine-Tune Sentence Transformers with Few Data with `setfit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b213a58",
   "metadata": {},
   "source": [
    "While LLMs get all the hype, we forget about the real achievements that matter to us.\n",
    "\n",
    "What if you could fine-tune Sentence Transformers for classification with 8 labelled examples per class?\n",
    "\n",
    "With **SetFit** for Python, you can fine-tune transformers in a few shots with high accuracy, without needing a huge labelled dataset.\n",
    "\n",
    "This is huge because getting labelled data is always hard, especially for text.\n",
    "\n",
    "Instead of looking at LLM benchmarks where big companies squeeze out 0.1 % performance out of it, look at the technologies that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7220f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "\n",
    "# Load a dataset from the Hugging Face Hub\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "# Simulate the few-shot regime by sampling 8 examples per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)\n",
    "eval_dataset = dataset[\"validation\"].select(range(100))\n",
    "test_dataset = dataset[\"validation\"].select(range(100, len(dataset[\"validation\"])))\n",
    "\n",
    "# Load a SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    labels=[\"negative\", \"positive\"],\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    batch_size=16,\n",
    "    num_epochs=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"sentence\": \"text\", \"label\": \"label\"}\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)\n",
    "# {'accuracy': 0.8691709844559585}\n",
    "\n",
    "# Push model to the Hub\n",
    "trainer.push_to_hub(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2\")\n",
    "\n",
    "# Download from Hub\n",
    "model = SetFitModel.from_pretrained(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2\")\n",
    "# Run inference\n",
    "preds = model.predict([\"i loved the spiderman movie!\", \"pineapple on pizza is the worst 🤮\"])\n",
    "print(preds)\n",
    "# [\"positive\", \"negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5d6f0",
   "metadata": {},
   "source": [
    "## Analyze Deep Learning Layers with **weightwatcher**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e91c8",
   "metadata": {},
   "source": [
    "How to detect if your Deep Learning model is overtrained?\n",
    "\n",
    "With **weightwatcher**\n",
    "\n",
    "**weightwatcher**, an open-source Python library, analyzes trained/pre-trained Deep Learning Models to detect potential problems or overparametrized layers.\n",
    "\n",
    "Without using training or test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weightwatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightwatcher as ww\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg19_bn(pretrained=True)\n",
    "watcher = ww.WeightWatcher(model=model)\n",
    "details = watcher.analyze()\n",
    "summary = watcher.get_summary(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc43044-aa11-4e72-bc12-97ab92290ef6",
   "metadata": {},
   "source": [
    "## Foundation Model For Tabular Data with `TabPFN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caf98d-0188-47d4-93d4-0adb6a7e5b98",
   "metadata": {},
   "source": [
    "How do you make zero-shot predictions using foundation models on tabular data?\n",
    "\n",
    "**TabPFN** from Prior Labs is a foundation model pre-trained on synthetic data to take datasets as input and make predictions.\n",
    "\n",
    "This means you can apply it to any unseen dataset.\n",
    "\n",
    "They claim to be better than traditional methods (which you should always check yourself on your dataset).\n",
    "\n",
    "But it is an interesting direction, as there may be domains with little to no data where traditional methods might fall behind, e.g. in healthcare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab141e-8bd2-46c3-91f5-03f6d259bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabpfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8df13-9ff2-483e-9b7e-3624f970e994",
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Initialize a classifier\n",
    "clf = TabPFNClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "prediction_probabilities = clf.predict_proba(X_test)\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, prediction_probabilities[:, 1]))\n",
    "\n",
    "# Predict labels\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Accuracy\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fcf95-1ab4-4524-a234-647d3860594a",
   "metadata": {},
   "source": [
    "## Fast Data Processing For AI Training with `litdata`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77256b50-a837-48f3-9109-8b4ea10703be",
   "metadata": {},
   "source": [
    "Struggling with slow data processing for AI training?\n",
    "\n",
    "Try **litdata**, the Python library that transforms and optimizes datasets at scale!\n",
    "\n",
    "**litdata** offers:\n",
    "🟢 Streaming large cloud datasets without local downloads\n",
    "🟢 Accelerating training by 20x with optimized data\n",
    "🟢 Parallelizing tasks across multiple machines effortlessly\n",
    "🟢 Securely handling sensitive data with advanced encryption\n",
    "\n",
    "Perfect for images, text, video, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7c9a9-1c7b-4fee-b648-3155d63de55d",
   "metadata": {},
   "source": [
    "!pip install litdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca44a97-dcc7-4edd-a042-e5e673784c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Format the dataset for fast loading\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import litdata as ld\n",
    "\n",
    "def random_images(index):\n",
    "    fake_images = Image.fromarray(np.random.randint(0, 256, (32, 32, 3), dtype=np.uint8))\n",
    "    fake_labels = np.random.randint(10)\n",
    "    data = {\"index\": index, \"image\": fake_images, \"class\": fake_labels}\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ld.optimize(\n",
    "        fn=random_images,\n",
    "        inputs=list(range(1000)),\n",
    "        output_dir=\"fast_data\",\n",
    "        num_workers=4,\n",
    "        chunk_bytes=\"64MB\"\n",
    "    )\n",
    "\n",
    "# Step 2: Put the data on the cloud\n",
    "$ aws s3 cp --recursive fast_data s3://my-bucket/fast_data\n",
    "\n",
    "# Step 3: Stream the data during training\n",
    "import litdata as ld\n",
    "\n",
    "train_dataset = ld.StreamingDataset('s3://my-bucket/fast_data', shuffle=True, drop_last=True)\n",
    "train_dataloader = ld.StreamingDataLoader(train_dataset)\n",
    "\n",
    "for sample in train_dataloader:\n",
    "    img, cls = sample['image'], sample['class'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64e665-e010-4cde-8880-5bdbb6dea7aa",
   "metadata": {},
   "source": [
    "## Faster Huggingface Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec4e11-3557-4966-8664-d80979d405d6",
   "metadata": {},
   "source": [
    "Struggling with slow model downloads from Hugging Face?\n",
    "\n",
    "Try **hf_transfer**, to go beyond 500MB/s.\n",
    "\n",
    "It maxes-out your CPU cores, making downloads much faster than the default.\n",
    "\n",
    "Note: It only makes a difference when you use it on a high bandwidth network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979cc94-847b-445b-ae48-3516147ffe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub[hf_transfer]\n",
    "!export HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74757a33-c228-49c4-91aa-c4f111b29d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"BAAI/bge-reranker-v2-m3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "54bf0f8d5625db32e314b5bdaf50a44046044c99ae376da8e1ac5bc25f06b01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
