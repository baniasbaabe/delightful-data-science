{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Prompts With No Loss with `llmlingua`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to reduce the costs of working with LLMs.\n",
    "\n",
    "When working with LLMs, we often encountered problems like exceeding token limits, forgetting context, or paying much more for usage than expected.\n",
    "\n",
    "Researchers from Microsoft try to solve these problems with `llmlingua`.\n",
    "\n",
    "`llmlingua` compresses your prompt by taking a trained small LLM to detect unimportant tokens.\n",
    "\n",
    "They claim to achieve up to 20x compression with no or minimal performance loss.\n",
    "\n",
    "I tried it out by myself and I noticed no performance loss at all, but I would be cautious for critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "prompt = \"<YOUR_PROMPT>\"\n",
    "llm_lingua = PromptCompressor(\"lgaalves/gpt2-dolly\",)\n",
    "\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# {'compressed_prompt': 'are- that turns into formatting & with like \"[]\" best it.......'\n",
    "# 'origin_tokens': 2430,\n",
    "# 'compressed_tokens': 261,\n",
    "# 'ratio': '9.3x',\n",
    "# 'saving': 'Saving $0.1 in GPT-4.}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Function Call to Any LLM with `litellm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want a One-Function call to any LLM in Python?\n",
    "\n",
    "Try `litellm`.\n",
    "\n",
    "`litellm` is a Python package to call any LLM in a consistent format and to return a consistent output.\n",
    "\n",
    "You only need to set the API key of the provider and the model name.\n",
    "\n",
    "It also supports async calls and streaming the models response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" \n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "os.environ['MISTRAL_API_KEY'] = \"your-api-key\"\n",
    "\n",
    "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "# Anthropic\n",
    "response = completion(model=\"claude-instant-1\", messages=messages)\n",
    "\n",
    "# Mistral\n",
    "response = completion(model=\"mistral/mistral-tiny\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguard Your LLMs with `LLMGuard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safeguarding your LLMs against unwanting behavior is critical.\n",
    "\n",
    "`LLMGuard`, a Python package, ensures a safe interaction between the user and LLM.\n",
    "\n",
    "It checks prompts and outputs for:\n",
    "\n",
    "- Sensitive Information like credit card number and sanitizes it\n",
    "- Toxic or harmful language\n",
    "- Prompt injections\n",
    "\n",
    "Use `LLMGuard` to make your LLMs more safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llm-guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from llm_guard import scan_output, scan_prompt\n",
    "from llm_guard.input_scanners import Anonymize, PromptInjection, Toxicity\n",
    "from llm_guard.vault import Vault\n",
    "\n",
    "client = OpenAI(api_key=\"OPENAIKEY\")\n",
    "vault = Vault()\n",
    "input_scanners = [Anonymize(vault), Toxicity(), PromptInjection()]\n",
    "\n",
    "prompt = \"Make an SQL insert statement to add a new user to our database. Name is John Doe. \\\n",
    "Email is test@test.com Phone number is 555-123-4567 and the IP address is 192.168.1.100. \\\n",
    "And credit card number is 4567-8901-2345-6789.\"\n",
    "\n",
    "sanitized_prompt, results_valid, results_score = scan_prompt(input_scanners, prompt)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": sanitized_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Sanitized Prompt: \n",
    "# Make an SQL insert statement to add a new user to our database. \n",
    "# Name is [REDACTED_PERSON_1]. Email is [REDACTED_EMAIL_ADDRESS_1] Phone number is \n",
    "# [REDACTED_PHONE_NUMBER_1] and the IP address is [REDACTED_IP_ADDRESS_1]. \n",
    "# And credit card number is [REDACTED_CREDIT_CARD_RE_1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LLMs with `uptrain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LLMs can be tricky.\n",
    "\n",
    "Luckily, `uptrain` offers a neat library to do that.\n",
    "\n",
    "`uptrain` is a Python library to evaluate LLMs with 20+ preconfigured checks.\n",
    "\n",
    "This includes the quality of the responses (completeness, validity,...), language proficiency (tonality, conciseness, ...) and code hallucination.\n",
    "\n",
    "It supports the biggest providers like OpenAI, Mistral, Claude and Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import EvalLLM, Evals\n",
    "import json\n",
    "\n",
    "OPENAI_API_KEY = \"*******\"\n",
    "\n",
    "data = [{\n",
    "    'question': 'Which is the most popular global sport?',\n",
    "    'context': \"The popularity of sports can be measured in various ways, including TV viewership...\",\n",
    "    'response': 'Football is the most popular sport with around 4 billion followers worldwide'\n",
    "}]\n",
    "\n",
    "eval_llm = EvalLLM(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "results = eval_llm.evaluate(\n",
    "    data=data,\n",
    "    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Any Type of File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, everything is about Embeddings and LLMs.\n",
    "\n",
    "The Python library `embed-anything` makes it easy to generate embeddings from multiple sources like image, video, or audio.\n",
    "\n",
    "It's built in Rust so it executes fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install embed-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embed_anything\n",
    "\n",
    "data = embed_anything.embed_file(\"filename.pdf\", embeder= \"Bert\")\n",
    "embeddings = np.array([data.embedding for data in data])\n",
    "\n",
    "data = embed_anything.embed_directory(\"test_files\", embeder= \"Clip\")\n",
    "embeddings = np.array([data.embedding for data in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured LLM Output with `outlines`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you annoyed of unstructured outputs of LLMs?\n",
    "\n",
    "Try `outlines`.\n",
    "\n",
    "`outlines` is a Python library for structured generation of your LLMs.\n",
    "\n",
    "There are multiple ways to enforce the output of the model like choices, type constraints, JSON output, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "prompt = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "\n",
    "generator = outlines.generate.choice(model, [\"Positive\", \"Negative\"])\n",
    "answer = generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG Pipelines with `Ragas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you evaluate your RAG application?\n",
    "\n",
    "Sure, you can look manually over your responses and see if it's what you want.\n",
    "\n",
    "But, it's not scalable.\n",
    "\n",
    "Instead, use `Ragas` in Python.\n",
    "\n",
    "`Ragas` is a library providing evaluation techniques and metrics for your RAG pipeline like Context Precision/Recall, Faithfulness and answer relevancy.\n",
    "\n",
    "See below how easy it is to run `Ragas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset \n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset,metrics=[faithfulness,answer_correctness])\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Reranker API with `rerankers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential element of your RAG systems is reranking.\n",
    "\n",
    "Reranking involves a reranking model that outputs a similarity score for each retrieved document and the user query.\n",
    "\n",
    "The `rerankers` library gives you a unified API to use with popular vendors and models such as Cohere, Jina or T5.\n",
    "\n",
    "The perfect API to easily test and replace many methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\"t5\")\n",
    "\n",
    "results = ranker.rank(query=\"I love you\", docs=[\"I hate you\", \"I really like you\"], doc_ids=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings on your CPU with `fastembed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My favourite library for creating embeddings:\n",
    "\n",
    "`fastembed`, developed by Qdrant.\n",
    "\n",
    "`fastembed` is a lightweight and fast library for using popular embedding models.\n",
    "\n",
    "Without using your GPU.\n",
    "\n",
    "It also integrates seamlessly with Qdrant's vector database.\n",
    "\n",
    "I would like to see more supported models though, as `fastembed` has so much potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "documents = [\n",
    "    \"This is some\",\n",
    "    \"example document\",\n",
    "]\n",
    "\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "embeddings = list(embedding_model.embed(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Files to Markdown & JSON with `docling`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing your data for LLMs is a crucial step in RAG applications.\n",
    "\n",
    "`docling` simplifies this step for you by converting popular document formats like PDF or PPT to Markdown or JSON.\n",
    "\n",
    "It uses two models, layout analyis model and table structure recognition model, to process the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2408.09869\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())  \n",
    "# Output: \"## Docling Technical Report[...]\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
