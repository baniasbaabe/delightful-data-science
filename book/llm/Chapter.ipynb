{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Prompts With No Loss with `llmlingua`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to reduce the costs of working with LLMs.\n",
    "\n",
    "When working with LLMs, we often encountered problems like exceeding token limits, forgetting context, or paying much more for usage than expected.\n",
    "\n",
    "Researchers from Microsoft try to solve these problems with `llmlingua`.\n",
    "\n",
    "`llmlingua` compresses your prompt by taking a trained small LLM to detect unimportant tokens.\n",
    "\n",
    "They claim to achieve up to 20x compression with no or minimal performance loss.\n",
    "\n",
    "I tried it out by myself and I noticed no performance loss at all, but I would be cautious for critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "prompt = \"<YOUR_PROMPT>\"\n",
    "llm_lingua = PromptCompressor(\"lgaalves/gpt2-dolly\",)\n",
    "\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# {'compressed_prompt': 'are- that turns into formatting & with like \"[]\" best it.......'\n",
    "# 'origin_tokens': 2430,\n",
    "# 'compressed_tokens': 261,\n",
    "# 'ratio': '9.3x',\n",
    "# 'saving': 'Saving $0.1 in GPT-4.}'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
