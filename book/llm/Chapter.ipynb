{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Prompts With No Loss with `llmlingua`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to reduce the costs of working with LLMs.\n",
    "\n",
    "When working with LLMs, we often encountered problems like exceeding token limits, forgetting context, or paying much more for usage than expected.\n",
    "\n",
    "Researchers from Microsoft try to solve these problems with `llmlingua`.\n",
    "\n",
    "`llmlingua` compresses your prompt by taking a trained small LLM to detect unimportant tokens.\n",
    "\n",
    "They claim to achieve up to 20x compression with no or minimal performance loss.\n",
    "\n",
    "I tried it out by myself and I noticed no performance loss at all, but I would be cautious for critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "prompt = \"<YOUR_PROMPT>\"\n",
    "llm_lingua = PromptCompressor(\"lgaalves/gpt2-dolly\",)\n",
    "\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# {'compressed_prompt': 'are- that turns into formatting & with like \"[]\" best it.......'\n",
    "# 'origin_tokens': 2430,\n",
    "# 'compressed_tokens': 261,\n",
    "# 'ratio': '9.3x',\n",
    "# 'saving': 'Saving $0.1 in GPT-4.}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Function Call to Any LLM with `litellm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want a One-Function call to any LLM in Python?\n",
    "\n",
    "Try `litellm`.\n",
    "\n",
    "`litellm` is a Python package to call any LLM in a consistent format and to return a consistent output.\n",
    "\n",
    "You only need to set the API key of the provider and the model name.\n",
    "\n",
    "It also supports async calls and streaming the models response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" \n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "os.environ['MISTRAL_API_KEY'] = \"your-api-key\"\n",
    "\n",
    "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "# Anthropic\n",
    "response = completion(model=\"claude-instant-1\", messages=messages)\n",
    "\n",
    "# Mistral\n",
    "response = completion(model=\"mistral/mistral-tiny\", messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safeguard Your LLMs with `LLMGuard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safeguarding your LLMs against unwanting behavior is critical.\n",
    "\n",
    "`LLMGuard`, a Python package, ensures a safe interaction between the user and LLM.\n",
    "\n",
    "It checks prompts and outputs for:\n",
    "\n",
    "- Sensitive Information like credit card number and sanitizes it\n",
    "- Toxic or harmful language\n",
    "- Prompt injections\n",
    "\n",
    "Use `LLMGuard` to make your LLMs more safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llm-guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from llm_guard import scan_output, scan_prompt\n",
    "from llm_guard.input_scanners import Anonymize, PromptInjection, Toxicity\n",
    "from llm_guard.vault import Vault\n",
    "\n",
    "client = OpenAI(api_key=\"OPENAIKEY\")\n",
    "vault = Vault()\n",
    "input_scanners = [Anonymize(vault), Toxicity(), PromptInjection()]\n",
    "\n",
    "prompt = \"Make an SQL insert statement to add a new user to our database. Name is John Doe. \\\n",
    "Email is test@test.com Phone number is 555-123-4567 and the IP address is 192.168.1.100. \\\n",
    "And credit card number is 4567-8901-2345-6789.\"\n",
    "\n",
    "sanitized_prompt, results_valid, results_score = scan_prompt(input_scanners, prompt)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": sanitized_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Sanitized Prompt: \n",
    "# Make an SQL insert statement to add a new user to our database. \n",
    "# Name is [REDACTED_PERSON_1]. Email is [REDACTED_EMAIL_ADDRESS_1] Phone number is \n",
    "# [REDACTED_PHONE_NUMBER_1] and the IP address is [REDACTED_IP_ADDRESS_1]. \n",
    "# And credit card number is [REDACTED_CREDIT_CARD_RE_1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LLMs with `uptrain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LLMs can be tricky.\n",
    "\n",
    "Luckily, `uptrain` offers a neat library to do that.\n",
    "\n",
    "`uptrain` is a Python library to evaluate LLMs with 20+ preconfigured checks.\n",
    "\n",
    "This includes the quality of the responses (completeness, validity,...), language proficiency (tonality, conciseness, ...) and code hallucination.\n",
    "\n",
    "It supports the biggest providers like OpenAI, Mistral, Claude and Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import EvalLLM, Evals\n",
    "import json\n",
    "\n",
    "OPENAI_API_KEY = \"*******\"\n",
    "\n",
    "data = [{\n",
    "    'question': 'Which is the most popular global sport?',\n",
    "    'context': \"The popularity of sports can be measured in various ways, including TV viewership...\",\n",
    "    'response': 'Football is the most popular sport with around 4 billion followers worldwide'\n",
    "}]\n",
    "\n",
    "eval_llm = EvalLLM(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "results = eval_llm.evaluate(\n",
    "    data=data,\n",
    "    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Any Type of File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, everything is about Embeddings and LLMs.\n",
    "\n",
    "The Python library `embed-anything` makes it easy to generate embeddings from multiple sources like image, video, or audio.\n",
    "\n",
    "It's built in Rust so it executes fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install embed-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embed_anything\n",
    "\n",
    "data = embed_anything.embed_file(\"filename.pdf\", embeder= \"Bert\")\n",
    "embeddings = np.array([data.embedding for data in data])\n",
    "\n",
    "data = embed_anything.embed_directory(\"test_files\", embeder= \"Clip\")\n",
    "embeddings = np.array([data.embedding for data in data])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
