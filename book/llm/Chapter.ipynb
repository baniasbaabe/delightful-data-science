{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Prompts With No Loss with `llmlingua`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to reduce the costs of working with LLMs.\n",
    "\n",
    "When working with LLMs, we often encountered problems like exceeding token limits, forgetting context, or paying much more for usage than expected.\n",
    "\n",
    "Researchers from Microsoft try to solve these problems with `llmlingua`.\n",
    "\n",
    "`llmlingua` compresses your prompt by taking a trained small LLM to detect unimportant tokens.\n",
    "\n",
    "They claim to achieve up to 20x compression with no or minimal performance loss.\n",
    "\n",
    "I tried it out by myself and I noticed no performance loss at all, but I would be cautious for critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "prompt = \"<YOUR_PROMPT>\"\n",
    "llm_lingua = PromptCompressor(\"lgaalves/gpt2-dolly\",)\n",
    "\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# {'compressed_prompt': 'are- that turns into formatting & with like \"[]\" best it.......'\n",
    "# 'origin_tokens': 2430,\n",
    "# 'compressed_tokens': 261,\n",
    "# 'ratio': '9.3x',\n",
    "# 'saving': 'Saving $0.1 in GPT-4.}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Function Call to Any LLM with `litellm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want a One-Function call to any LLM in Python?\n",
    "\n",
    "Try `litellm`.\n",
    "\n",
    "`litellm` is a Python package to call any LLM in a consistent format and to return a consistent output.\n",
    "\n",
    "You only need to set the API key of the provider and the model name.\n",
    "\n",
    "It also supports async calls and streaming the models response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" \n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "os.environ['MISTRAL_API_KEY'] = \"your-api-key\"\n",
    "\n",
    "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "# Anthropic\n",
    "response = completion(model=\"claude-instant-1\", messages=messages)\n",
    "\n",
    "# Anthropic\n",
    "response = completion(model=\"mistral/mistral-tiny\", messages=messages)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
